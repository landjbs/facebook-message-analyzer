{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Message Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Machine Learning to Determine Friendship from a Sentence: </b>\n",
    "Friendship is measured by whether or not number of messages is over cutoff. After processing the data, it seems the relationship between words and friendship is quite thin. However, my keras model does what it can with the data and the user interface at the very bottom lets you test sample messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/landonsmith/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS AND VARIABLES\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "CURRENT_DIRECTORY = os.getcwd()\n",
    "NUMBER_TO_ANALYZE = 50000\n",
    "MESSAGE_THRESHOLD = 100\n",
    "FRIEND_CUTOFF = 1000 # number of messages with a person to consider a friend\n",
    "WORD_USE_CUTOFF = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_data(chat):\n",
    "    try:\n",
    "        json_location = CURRENT_DIRECTORY + \"/messages/\" + chat + \"/message.json\"\n",
    "        with open(json_location) as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "            return json_data\n",
    "    except IOError:\n",
    "        pass # some things the directory aren't messages (DS_Store, stickers_used, etc.)\n",
    "\n",
    "def clean_word(word):\n",
    "    return (word.lower())\n",
    "\n",
    "# ANALYZE CHATS\n",
    "chats = os.listdir(CURRENT_DIRECTORY + \"/messages/\")[:NUMBER_TO_ANALYZE]\n",
    "sorted_chats = []\n",
    "final_data_messages = {}\n",
    "final_data_times = {}\n",
    "final_data_words = {}\n",
    "invalid_message_count = 0\n",
    "\n",
    "for chat in chats:\n",
    "    url = chat + '/message.json'\n",
    "    json_data = get_json_data(chat)\n",
    "    if json_data != None:\n",
    "        messages = json_data[\"messages\"]\n",
    "        if len(messages) >= MESSAGE_THRESHOLD:\n",
    "            sorted_chats.append((len(messages), chat, messages))\n",
    "\n",
    "sorted_chats.sort(reverse=True)\n",
    "\n",
    "words_used = []\n",
    "message_words = []\n",
    "number_messages = []\n",
    "\n",
    "for i, (messages, chat, messages) in enumerate(sorted_chats):\n",
    "    for message in messages:\n",
    "        try:\n",
    "            # is person a friend?\n",
    "            friend_binary = 1 if len(messages) > FRIEND_CUTOFF else 0\n",
    "            number_messages.append(friend_binary)\n",
    "            # strings used in message\n",
    "            words_list = [clean_word(word) for word in message[\"content\"].split()]\n",
    "            words_used += words_list\n",
    "            message_words.append(words_list)\n",
    "        except KeyError:\n",
    "            # happens for special cases like users who deactivated, unfriended, blocked\n",
    "            invalid_message_count += 1\n",
    "# make set of words used more than 10 and less than 400 times\n",
    "times_used = Counter(words_used)\n",
    "significant_words = set([k for k,v in times_used.items() if v > 10 and v < 400])\n",
    "\n",
    "# matrix to hold word vector for each message\n",
    "usage_matrix = np.zeros((len(message_words), len(significant_words)))\n",
    "\n",
    "friend_vector = []\n",
    "# create binary usage vector for words in each message\n",
    "for rowCounter, message in enumerate(message_words):\n",
    "    friend_vector.append(number_messages[rowCounter])\n",
    "    for word in message:\n",
    "        for colCounter, word_check in enumerate(significant_words):\n",
    "            if word == word_check:\n",
    "                usage_matrix[rowCounter, colCounter] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "15181/15181 [==============================] - 1s 45us/step - loss: 7.2713 - acc: 0.5439\n",
      "Epoch 2/3\n",
      "15181/15181 [==============================] - 0s 24us/step - loss: 7.2713 - acc: 0.5439\n",
      "Epoch 3/3\n",
      "15181/15181 [==============================] - 0s 24us/step - loss: 7.2713 - acc: 0.5439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2f984c50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEURAL NET TRAINING AND SET UP\n",
    "df = pd.DataFrame(data=usage_matrix)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(len(significant_words),)),\n",
    "    Activation('relu'),\n",
    "    Dense(1),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(df, friend_vector, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send me a sample message:\n",
      "what up dude\n",
      "I think we're likely to have lots of messages!\n"
     ]
    }
   ],
   "source": [
    "# CHECK AGAINST MODEL\n",
    "def check_against_model():\n",
    "    user_words = input(\"Send me a sample message:\\n\")\n",
    "    cleaned_input = [clean_word(word) for word in user_words.split()]\n",
    "    input_vector = np.zeros((len(significant_words)),)\n",
    "    for word in cleaned_input:\n",
    "        for count, word_check in enumerate(significant_words):\n",
    "            if word == word_check:\n",
    "                input_vector[count] = 1\n",
    "    predictionDF = pd.DataFrame(input_vector)\n",
    "    result = model.predict(predictionDF.T)\n",
    "    if result == 1:\n",
    "        print(\"I think we're likely to have lots of messages!\")\n",
    "    else:\n",
    "        print(\"We probably don't have many messages :(\")\n",
    "\n",
    "check_against_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
